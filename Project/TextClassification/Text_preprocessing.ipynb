{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acb0ffd7",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047fbe23",
   "metadata": {},
   "source": [
    "### 1. Tokenization: \n",
    "\n",
    "Tokenization is the process of breaking down a text into individual words, phrases, or symbols. In Python, you can use the NLTK or spaCy libraries for tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c19f86e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'prompt_toolkit.formatted_text'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"This is PGDBA Data Science Class.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30e40749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('PGDBA', 'NNP'),\n",
       " ('Data', 'NNP'),\n",
       " ('Science', 'NNP'),\n",
       " ('Class', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a038628",
   "metadata": {},
   "source": [
    "### 2. Punctuation and Stop word removal: \n",
    "\n",
    "Stop words are common words that are usually removed from text because they don't carry much meaning, such as \"a\", \"an\", \"the\", \"in\", \"of\", etc. You can use the NLTK library to remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3628226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is PGDBA Data Science Class\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "no_punct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "print(no_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67b3ccc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "005129ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pgdba', 'data', 'science', 'class', '.']\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_tokens = \"\".join([word.lower() for word in tokens if not word.lower() in stop_words])\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63b787e",
   "metadata": {},
   "source": [
    "### 3. Stemming and Lemmatization: \n",
    "Stemming and Lemmatization are techniques used to reduce words to their base form (or root form) so that similar words can be grouped together. In Python, you can use the NLTK or spaCy libraries for stemming and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fc23091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rock', 'rock', 'rocking', 'history', 'geography', 'happy', 'happyly']\n",
      "['rock', 'rock', 'rock', 'histori', 'geographi', 'happi', 'happyli']\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "text = \"rock rocks rocking history geography happy happyly\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "print(lemmatized_tokens)\n",
    "\n",
    "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97b702f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25cd98a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
